{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CekAjaYuk Model Retraining\n",
    "## Dokumentasi Pelatihan Ulang Model dengan Dataset Real\n",
    "\n",
    "**Dataset**: 400 fake + 400 genuine job posting images\n",
    "**Tujuan**: Meningkatkan akurasi deteksi fake job posting\n",
    "**Models**: Random Forest, Text Classifier, CNN, OCR Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_DIR = Path('dataset')\n",
    "FAKE_DIR = DATASET_DIR / 'fake'\n",
    "GENUINE_DIR = DATASET_DIR / 'genuine'\n",
    "\n",
    "# Check dataset\n",
    "fake_images = list(FAKE_DIR.glob('*.jpg')) if FAKE_DIR.exists() else []\n",
    "genuine_images = list(GENUINE_DIR.glob('*.JPG')) if GENUINE_DIR.exists() else []\n",
    "\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"   Fake images: {len(fake_images)}\")\n",
    "print(f\"   Genuine images: {len(genuine_images)}\")\n",
    "print(f\"   Total images: {len(fake_images) + len(genuine_images)}\")\n",
    "\n",
    "# Visualize dataset distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Fake', 'Genuine']\n",
    "sizes = [len(fake_images), len(genuine_images)]\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Dataset Distribution')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Dataset balanced: {abs(len(fake_images) - len(genuine_images)) <= 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OCR Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OCR function from backend\n",
    "sys.path.append('.')\n",
    "try:\n",
    "    from backend_working import extract_text_with_ocr\n",
    "    print(\"‚úÖ OCR function imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing OCR function: {e}\")\n",
    "    print(\"Please ensure backend_working.py is in the same directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from all images\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def extract_text_from_images(image_paths, label, max_images=100):\n",
    "    \"\"\"Extract text from images with progress tracking\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"üîç Extracting text from {min(len(image_paths), max_images)} {label} images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths[:max_images]):\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            # Extract text\n",
    "            text = extract_text_with_ocr(image)\n",
    "            \n",
    "            if len(text.strip()) > 10:  # Only include if meaningful text extracted\n",
    "                texts.append(text.strip())\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                failed_count += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"   Processed {i + 1}/{min(len(image_paths), max_images)} images...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to process {img_path.name}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"   ‚úÖ Successfully extracted: {len(texts)} texts\")\n",
    "    print(f\"   ‚ùå Failed extractions: {failed_count}\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Extract text from fake images\n",
    "fake_texts, fake_labels = extract_text_from_images(fake_images, 'fake', max_images=100)\n",
    "\n",
    "# Extract text from genuine images\n",
    "genuine_texts, genuine_labels = extract_text_from_images(genuine_images, 'genuine', max_images=100)\n",
    "\n",
    "# Combine all data\n",
    "all_texts = fake_texts + genuine_texts\n",
    "all_labels = fake_labels + genuine_labels\n",
    "\n",
    "print(f\"\\nüìä Text Extraction Summary:\")\n",
    "print(f\"   Total texts extracted: {len(all_texts)}\")\n",
    "print(f\"   Fake texts: {len(fake_texts)}\")\n",
    "print(f\"   Genuine texts: {len(genuine_texts)}\")\n",
    "print(f\"   Average text length: {np.mean([len(text) for text in all_texts]):.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics\n",
    "def analyze_text_characteristics(texts, labels):\n",
    "    \"\"\"Analyze characteristics of fake vs genuine texts\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'length': [len(text) for text in texts],\n",
    "        'word_count': [len(text.split()) for text in texts]\n",
    "    })\n",
    "    \n",
    "    print(\"üìä Text Characteristics Analysis:\")\n",
    "    print(df.groupby('label')[['length', 'word_count']].describe())\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Text length distribution\n",
    "    df.boxplot(column='length', by='label', ax=axes[0])\n",
    "    axes[0].set_title('Text Length Distribution')\n",
    "    axes[0].set_xlabel('Label')\n",
    "    axes[0].set_ylabel('Character Count')\n",
    "    \n",
    "    # Word count distribution\n",
    "    df.boxplot(column='word_count', by='label', ax=axes[1])\n",
    "    axes[1].set_title('Word Count Distribution')\n",
    "    axes[1].set_xlabel('Label')\n",
    "    axes[1].set_ylabel('Word Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze text characteristics\n",
    "text_df = analyze_text_characteristics(all_texts, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced keyword lists for Indonesian job postings\n",
    "GENUINE_KEYWORDS = [\n",
    "    # Professional terms\n",
    "    'pengalaman', 'kualifikasi', 'syarat', 'tanggung jawab', 'tunjangan',\n",
    "    'gaji', 'wawancara', 'lamaran', 'kandidat', 'posisi', 'lowongan',\n",
    "    'kerja', 'pekerjaan', 'perusahaan', 'pt', 'cv', 'tbk', 'profesional',\n",
    "    'karir', 'jabatan', 'keahlian', 'kemampuan', 'keterampilan',\n",
    "    'pendidikan', 'gelar', 'ijazah', 'sertifikat', 'pelatihan',\n",
    "    \n",
    "    # Job-related terms\n",
    "    'interview', 'recruitment', 'hiring', 'vacancy', 'position',\n",
    "    'experience', 'qualification', 'requirement', 'responsibility',\n",
    "    'salary', 'benefit', 'career', 'professional', 'skill',\n",
    "    \n",
    "    # Company terms\n",
    "    'kantor', 'office', 'company', 'corporation', 'enterprise',\n",
    "    'industri', 'bisnis', 'organisasi', 'institusi', 'lembaga'\n",
    "]\n",
    "\n",
    "FAKE_KEYWORDS = [\n",
    "    # Suspicious terms\n",
    "    'mudah', 'cepat', 'instant', 'langsung', 'tanpa modal',\n",
    "    'kerja rumah', 'work from home', 'online', 'part time',\n",
    "    'freelance', 'sampingan', 'tambahan', 'passive income',\n",
    "    'jutaan', 'milyar', 'unlimited', 'tak terbatas',\n",
    "    \n",
    "    # MLM/Scam indicators\n",
    "    'join', 'member', 'downline', 'upline', 'bonus',\n",
    "    'komisi', 'reward', 'cashback', 'investasi', 'trading',\n",
    "    'forex', 'crypto', 'bitcoin', 'saham', 'properti',\n",
    "    \n",
    "    # Urgency terms\n",
    "    'buruan', 'terbatas', 'deadline', 'segera', 'cepat',\n",
    "    'jangan sampai', 'terlewat', 'kesempatan emas'\n",
    "]\n",
    "\n",
    "def extract_features(texts):\n",
    "    \"\"\"Extract comprehensive features from texts\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Basic features\n",
    "        feature_dict = {\n",
    "            'length': len(text),\n",
    "            'word_count': len(text.split()),\n",
    "            'sentence_count': len([s for s in text.split('.') if s.strip()]),\n",
    "            'avg_word_length': np.mean([len(word) for word in text.split()]) if text.split() else 0,\n",
    "        }\n",
    "        \n",
    "        # Keyword features\n",
    "        genuine_count = sum(1 for kw in GENUINE_KEYWORDS if kw in text_lower)\n",
    "        fake_count = sum(1 for kw in FAKE_KEYWORDS if kw in text_lower)\n",
    "        \n",
    "        feature_dict.update({\n",
    "            'genuine_keywords': genuine_count,\n",
    "            'fake_keywords': fake_count,\n",
    "            'keyword_ratio': genuine_count / max(fake_count, 1),\n",
    "        })\n",
    "        \n",
    "        # Structure features\n",
    "        feature_dict.update({\n",
    "            'has_email': '@' in text,\n",
    "            'has_phone': any(char.isdigit() for char in text),\n",
    "            'has_address': any(word in text_lower for word in ['jl', 'jalan', 'street', 'alamat']),\n",
    "            'has_company': any(word in text_lower for word in ['pt', 'cv', 'ltd', 'inc', 'corp']),\n",
    "        })\n",
    "        \n",
    "        features.append(feature_dict)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract features\n",
    "print(\"üîß Extracting features from texts...\")\n",
    "features_df = extract_features(all_texts)\n",
    "features_df['label'] = all_labels\n",
    "\n",
    "print(f\"‚úÖ Features extracted: {features_df.shape[1]-1} features for {features_df.shape[0]} samples\")\n",
    "print(\"\\nüìä Feature Summary:\")\n",
    "print(features_df.groupby('label').mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = features_df.drop(['label'], axis=1)\n",
    "y = features_df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Training Data Split:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"\\n   Training distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\n   Testing distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Model\n",
    "print(\"üå≤ Training Random Forest Classifier...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Accuracy: {rf_accuracy:.3f}\")\n",
    "print(\"\\nüìä Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîç Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, 'models/random_forest_retrained.pkl')\n",
    "print(\"üíæ Random Forest model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Text Classifier (TF-IDF + Logistic Regression)\n",
    "print(\"üìù Training Text Classifier...\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words=None,  # Keep Indonesian words\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "# Split texts for TF-IDF\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "    all_texts, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "X_tfidf_train = tfidf.fit_transform(texts_train)\n",
    "X_tfidf_test = tfidf.transform(texts_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "text_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "text_model.fit(X_tfidf_train, labels_train)\n",
    "text_pred = text_model.predict(X_tfidf_test)\n",
    "text_accuracy = accuracy_score(labels_test, text_pred)\n",
    "\n",
    "print(f\"‚úÖ Text Classifier Accuracy: {text_accuracy:.3f}\")\n",
    "print(\"\\nüìä Text Classifier Classification Report:\")\n",
    "print(classification_report(labels_test, text_pred))\n",
    "\n",
    "# Save models\n",
    "joblib.dump(text_model, 'models/text_classifier_retrained.pkl')\n",
    "joblib.dump(tfidf, 'models/tfidf_vectorizer_retrained.pkl')\n",
    "print(\"üíæ Text Classifier models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "model_results = {\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'Text Classifier': text_accuracy\n",
    "}\n",
    "\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "for model_name, accuracy in model_results.items():\n",
    "    print(f\"   {model_name}: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = list(model_results.keys())\n",
    "accuracies = list(model_results.values())\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=['#4ecdc4', '#45b7d1'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_pred)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Random Forest Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Text Classifier confusion matrix\n",
    "text_cm = confusion_matrix(labels_test, text_pred)\n",
    "sns.heatmap(text_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Text Classifier Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Integration and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model integration code\n",
    "integration_code = '''\n",
    "# Updated model integration for backend_working.py\n",
    "# Replace the existing model functions with these improved versions\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load retrained models\n",
    "try:\n",
    "    rf_model_retrained = joblib.load('models/random_forest_retrained.pkl')\n",
    "    text_model_retrained = joblib.load('models/text_classifier_retrained.pkl')\n",
    "    tfidf_retrained = joblib.load('models/tfidf_vectorizer_retrained.pkl')\n",
    "    print(\"‚úÖ Retrained models loaded successfully\")\n",
    "except:\n",
    "    print(\"‚ùå Using fallback models\")\n",
    "    rf_model_retrained = None\n",
    "    text_model_retrained = None\n",
    "    tfidf_retrained = None\n",
    "\n",
    "def analyze_with_retrained_random_forest(text, text_features):\n",
    "    \"\"\"Improved Random Forest analysis with retrained model\"\"\"\n",
    "    try:\n",
    "        if rf_model_retrained is None:\n",
    "            return analyze_with_random_forest_detailed(text, text_features)  # Fallback\n",
    "        \n",
    "        # Extract features in same format as training\n",
    "        features = extract_features_for_model(text)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = rf_model_retrained.predict([features])[0]\n",
    "        probabilities = rf_model_retrained.predict_proba([features])[0]\n",
    "        \n",
    "        # Get confidence\n",
    "        if prediction == 'fake':\n",
    "            confidence = probabilities[0] * 100  # Probability of fake\n",
    "        else:\n",
    "            confidence = probabilities[1] * 100  # Probability of genuine\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'confidence': round(confidence, 1),\n",
    "            'reasoning': [f\"Retrained Random Forest prediction: {prediction}\"],\n",
    "            'model_name': 'Random Forest (Retrained)',\n",
    "            'features_analyzed': list(features.keys())\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return analyze_with_random_forest_detailed(text, text_features)  # Fallback\n",
    "\n",
    "def analyze_with_retrained_text_classifier(text):\n",
    "    \"\"\"Improved Text Classifier with retrained model\"\"\"\n",
    "    try:\n",
    "        if text_model_retrained is None or tfidf_retrained is None:\n",
    "            return analyze_with_text_classifier_detailed(text)  # Fallback\n",
    "        \n",
    "        # Vectorize text\n",
    "        text_vector = tfidf_retrained.transform([text])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = text_model_retrained.predict(text_vector)[0]\n",
    "        probabilities = text_model_retrained.predict_proba(text_vector)[0]\n",
    "        \n",
    "        # Get confidence\n",
    "        if prediction == 'fake':\n",
    "            confidence = probabilities[0] * 100\n",
    "        else:\n",
    "            confidence = probabilities[1] * 100\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'confidence': round(confidence, 1),\n",
    "            'reasoning': [f\"Retrained Text Classifier prediction: {prediction}\"],\n",
    "            'model_name': 'Text Classifier (Retrained)',\n",
    "            'features_analyzed': ['tfidf_features']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return analyze_with_text_classifier_detailed(text)  # Fallback\n",
    "'''\n",
    "\n",
    "# Save integration code\n",
    "with open('model_integration_code.py', 'w') as f:\n",
    "    f.write(integration_code)\n",
    "\n",
    "print(\"üíæ Model integration code saved to 'model_integration_code.py'\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. Create 'models' directory if it doesn't exist\")\n",
    "print(\"2. Run this notebook to train and save models\")\n",
    "print(\"3. Integrate the retrained models into backend_working.py\")\n",
    "print(\"4. Test the improved accuracy\")"
   ]
  }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
